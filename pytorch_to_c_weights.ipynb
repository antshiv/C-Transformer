{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7cfa79b-4cde-4c0a-8b4b-ebd05affc1d0",
   "metadata": {},
   "source": [
    "# Get the GPT-2 checkpoints to use in our c-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93df740-cf66-42ac-b027-89eb26bf7a79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedcd99e-939a-4402-968c-31184b665b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import struct\n",
    "from transformers import GPT2Model\n",
    "import hashlib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7effc9b-cae9-4d25-8879-d61fe46df1c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Python script to convert weights\n",
    "import torch\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "\n",
    "# Choose model size: 'gpt2' (124M), 'gpt2-medium' (355M), 'gpt2-large' (774M), 'gpt2-xl' (1.5B)\n",
    "model_name = 'gpt2'\n",
    "model = GPT2Model.from_pretrained(model_name)\n",
    "\n",
    "# Save weights\n",
    "torch.save(model.state_dict(), f'{model_name}_weights.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c64f49-2fdb-4451-bc66-0c1778a8d567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb34f1c-83b9-4ca5-941a-d9668e76ecf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Match C alignment constants\n",
    "CACHE_ALIGN = 64  # bytes\n",
    "FLOAT_SIZE = 4    # sizeof(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9a652-3c8a-457c-8bbf-a1b96497b8e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def align_up(n, alignment=CACHE_ALIGN):\n",
    "    \"\"\"Align number of floats to cache boundary\"\"\"\n",
    "    bytes_needed = n * FLOAT_SIZE\n",
    "    aligned_bytes = ((bytes_needed + alignment - 1) // alignment) * alignment\n",
    "    return aligned_bytes // FLOAT_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0562d10-8ad0-4e5e-bb6c-1711527c5cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_bump_weights(model_name='gpt2', output_file='gpt2_bump.weights'):\n",
    "    \"\"\"\n",
    "    Convert PyTorch model to bump-aligned weight format.\n",
    "    The output file layout EXACTLY matches the C memory layout.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Loading {model_name} from HuggingFace...\")\n",
    "    model = GPT2Model.from_pretrained(model_name)\n",
    "    state_dict = model.state_dict()\n",
    "    config = model.config\n",
    "    \n",
    "    # Extract dimensions\n",
    "    n_layers = config.n_layer\n",
    "    n_heads = config.n_head\n",
    "    embed_dim = config.n_embd\n",
    "    vocab_size = config.vocab_size\n",
    "    context_len = config.n_positions\n",
    "    head_dim = embed_dim // n_heads\n",
    "    \n",
    "    # Calculate aligned dimensions (matching C code)\n",
    "    aligned_embed_dim = align_up(embed_dim)\n",
    "    aligned_head_dim = align_up(head_dim)\n",
    "    aligned_context = align_up(context_len)\n",
    "    \n",
    "    print(f\"\\nModel Configuration:\")\n",
    "    print(f\"  Layers:              {n_layers}\")\n",
    "    print(f\"  Embedding:           {embed_dim} -> aligned: {aligned_embed_dim * FLOAT_SIZE} bytes\")\n",
    "    print(f\"  Heads:               {n_heads}\")\n",
    "    print(f\"  Head dim:            {head_dim} -> aligned: {aligned_head_dim * FLOAT_SIZE} bytes\")\n",
    "    print(f\"  Vocab:               {vocab_size}\")\n",
    "    print(f\"  Context:             {context_len} -> aligned: {aligned_context * FLOAT_SIZE} bytes\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # HEADER STRUCTURE (128 bytes total for nice alignment)\n",
    "    # ============================================================================\n",
    "    # Magic:           8 bytes   \"BUMPWGT2\"\n",
    "    # Version:         4 bytes   (2)\n",
    "    # Model Type:      4 bytes   (0=GPT2, 1=LLAMA, 2=MISTRAL, etc)\n",
    "    # Hyperparams:     6 * 4 = 24 bytes\n",
    "    # Aligned dims:    3 * 8 = 24 bytes (using size_t/uint64)\n",
    "    # Checksum:        32 bytes  (SHA256)\n",
    "    # Reserved:        32 bytes  (for future use)\n",
    "    # ============================================================================\n",
    "    \n",
    "    with open(output_file, 'wb') as f:\n",
    "        # Reserve space for header (will write it at the end with checksum)\n",
    "        header_size = 128\n",
    "        f.write(b'\\x00' * header_size)\n",
    "        \n",
    "        # Track what we're writing for verification\n",
    "        weight_map = {}\n",
    "        current_offset = header_size\n",
    "        \n",
    "        # Helper function to write aligned tensor\n",
    "        def write_tensor_aligned(tensor, name, expected_floats):\n",
    "            nonlocal current_offset\n",
    "            \n",
    "            # Convert to numpy\n",
    "            if hasattr(tensor, 'detach'):\n",
    "                data = tensor.detach().cpu().numpy().astype(np.float32)\n",
    "            else:\n",
    "                data = tensor.astype(np.float32)\n",
    "            \n",
    "            # Flatten\n",
    "            flat = data.flatten()\n",
    "            actual_floats = len(flat)\n",
    "            \n",
    "            # Create aligned buffer with zeros for padding\n",
    "            aligned_buffer = np.zeros(expected_floats, dtype=np.float32)\n",
    "            aligned_buffer[:actual_floats] = flat\n",
    "            \n",
    "            # Write to file\n",
    "            bytes_written = f.write(aligned_buffer.tobytes())\n",
    "            \n",
    "            # Record in map\n",
    "            weight_map[name] = {\n",
    "                'offset': current_offset,\n",
    "                'actual_floats': actual_floats,\n",
    "                'aligned_floats': expected_floats,\n",
    "                'bytes': bytes_written,\n",
    "                'shape': list(data.shape)\n",
    "            }\n",
    "            \n",
    "            current_offset += bytes_written\n",
    "            print(f\"  {name}: {data.shape} -> {expected_floats} floats ({bytes_written} bytes)\")\n",
    "            \n",
    "            return bytes_written\n",
    "        \n",
    "        total_bytes = 0\n",
    "        \n",
    "        # ============================================================================\n",
    "        # WRITE WEIGHTS IN EXACT BUMP ORDER\n",
    "        # This order must match layout_transformer() in C exactly!\n",
    "        # ============================================================================\n",
    "        \n",
    "        print(\"\\n📝 Writing embeddings...\")\n",
    "        \n",
    "        # 1. Token embeddings [vocab_size × aligned_embed_dim]\n",
    "        write_tensor_aligned(\n",
    "            state_dict['wte.weight'],\n",
    "            'token_embeddings',\n",
    "            vocab_size * aligned_embed_dim\n",
    "        )\n",
    "        \n",
    "        # 2. Position embeddings [context_len × aligned_embed_dim]\n",
    "        write_tensor_aligned(\n",
    "            state_dict['wpe.weight'],\n",
    "            'position_embeddings',\n",
    "            context_len * aligned_embed_dim\n",
    "        )\n",
    "        \n",
    "        # 3. Skip embedded_input_offset (runtime buffer, not weights)\n",
    "        # In C: M->embedded_input_offset = bump(&off, context * aligned_embed_dim, CACHE_ALIGN)\n",
    "        # We don't write this - it's an activation buffer\n",
    "        \n",
    "        print(\"\\n🔧 Writing layer weights...\")\n",
    "        \n",
    "        # 4. Layer weights\n",
    "        for layer_idx in range(n_layers):\n",
    "            print(f\"\\n  Layer {layer_idx + 1}/{n_layers}:\")\n",
    "            prefix = f'h.{layer_idx}'\n",
    "            \n",
    "            # Skip layer_start_canary_offset (debug marker, not weights)\n",
    "            \n",
    "            # LayerNorm 1\n",
    "            write_tensor_aligned(\n",
    "                state_dict[f'{prefix}.ln_1.weight'],\n",
    "                f'layer_{layer_idx}_ln1_weight',\n",
    "                aligned_embed_dim\n",
    "            )\n",
    "            write_tensor_aligned(\n",
    "                state_dict[f'{prefix}.ln_1.bias'],\n",
    "                f'layer_{layer_idx}_ln1_bias',\n",
    "                aligned_embed_dim\n",
    "            )\n",
    "            \n",
    "            # Skip ln1_mean_offset, ln1_rstd_offset (runtime buffers)\n",
    "            # Skip layer_input_offset, ln1_output_offset (runtime buffers)\n",
    "            \n",
    "            # QKV weights - need to split from combined tensor\n",
    "            qkv_weight = state_dict[f'{prefix}.attn.c_attn.weight'].T  # Transpose for C\n",
    "            qkv_bias = state_dict[f'{prefix}.attn.c_attn.bias']\n",
    "            \n",
    "            # Split QKV (they're concatenated in dim 0)\n",
    "            q_weight = qkv_weight[:embed_dim, :]\n",
    "            k_weight = qkv_weight[embed_dim:2*embed_dim, :]\n",
    "            v_weight = qkv_weight[2*embed_dim:3*embed_dim, :]\n",
    "            \n",
    "            q_bias = qkv_bias[:embed_dim]\n",
    "            k_bias = qkv_bias[embed_dim:2*embed_dim]\n",
    "            v_bias = qkv_bias[2*embed_dim:3*embed_dim]\n",
    "            \n",
    "            # Q weights and bias\n",
    "            write_tensor_aligned(\n",
    "                q_weight,\n",
    "                f'layer_{layer_idx}_q_weight',\n",
    "                aligned_embed_dim * aligned_embed_dim\n",
    "            )\n",
    "            write_tensor_aligned(\n",
    "                q_bias,\n",
    "                f'layer_{layer_idx}_q_bias',\n",
    "                aligned_embed_dim\n",
    "            )\n",
    "            # Skip q_output_offset (runtime buffer)\n",
    "            \n",
    "            # K weights and bias\n",
    "            write_tensor_aligned(\n",
    "                k_weight,\n",
    "                f'layer_{layer_idx}_k_weight',\n",
    "                aligned_embed_dim * aligned_embed_dim\n",
    "            )\n",
    "            write_tensor_aligned(\n",
    "                k_bias,\n",
    "                f'layer_{layer_idx}_k_bias',\n",
    "                aligned_embed_dim\n",
    "            )\n",
    "            # Skip k_output_offset (runtime buffer)\n",
    "            \n",
    "            # V weights and bias\n",
    "            write_tensor_aligned(\n",
    "                v_weight,\n",
    "                f'layer_{layer_idx}_v_weight',\n",
    "                aligned_embed_dim * aligned_embed_dim\n",
    "            )\n",
    "            write_tensor_aligned(\n",
    "                v_bias,\n",
    "                f'layer_{layer_idx}_v_bias',\n",
    "                aligned_embed_dim\n",
    "            )\n",
    "            # Skip v_output_offset (runtime buffer)\n",
    "            \n",
    "            # Skip attention_scores_offset (runtime buffer)\n",
    "            \n",
    "            # Projection weights\n",
    "            proj_weight = state_dict[f'{prefix}.attn.c_proj.weight'].T  # Transpose\n",
    "            proj_bias = state_dict[f'{prefix}.attn.c_proj.bias']\n",
    "            \n",
    "            write_tensor_aligned(\n",
    "                proj_weight,\n",
    "                f'layer_{layer_idx}_proj_weight',\n",
    "                aligned_embed_dim * aligned_embed_dim\n",
    "            )\n",
    "            write_tensor_aligned(\n",
    "                proj_bias,\n",
    "                f'layer_{layer_idx}_proj_bias',\n",
    "                aligned_embed_dim\n",
    "            )\n",
    "            \n",
    "            # Skip attention_output_offset, residual1_output_offset (runtime buffers)\n",
    "            \n",
    "            # LayerNorm 2\n",
    "            write_tensor_aligned(\n",
    "                state_dict[f'{prefix}.ln_2.weight'],\n",
    "                f'layer_{layer_idx}_ln2_weight',\n",
    "                aligned_embed_dim\n",
    "            )\n",
    "            write_tensor_aligned(\n",
    "                state_dict[f'{prefix}.ln_2.bias'],\n",
    "                f'layer_{layer_idx}_ln2_bias',\n",
    "                aligned_embed_dim\n",
    "            )\n",
    "            \n",
    "            # Skip ln2_mean_offset, ln2_rstd_offset, ln2_output_offset (runtime buffers)\n",
    "            \n",
    "            # MLP weights\n",
    "            fc1_weight = state_dict[f'{prefix}.mlp.c_fc.weight'].T  # Transpose\n",
    "            fc1_bias = state_dict[f'{prefix}.mlp.c_fc.bias']\n",
    "            \n",
    "            write_tensor_aligned(\n",
    "                fc1_weight,\n",
    "                f'layer_{layer_idx}_fc1_weight',\n",
    "                4 * aligned_embed_dim * aligned_embed_dim\n",
    "            )\n",
    "            write_tensor_aligned(\n",
    "                fc1_bias,\n",
    "                f'layer_{layer_idx}_fc1_bias',\n",
    "                4 * aligned_embed_dim\n",
    "            )\n",
    "            # Skip fc1_output_offset (runtime buffer)\n",
    "            \n",
    "            fc2_weight = state_dict[f'{prefix}.mlp.c_proj.weight'].T  # Transpose\n",
    "            fc2_bias = state_dict[f'{prefix}.mlp.c_proj.bias']\n",
    "            \n",
    "            write_tensor_aligned(\n",
    "                fc2_weight,\n",
    "                f'layer_{layer_idx}_fc2_weight',\n",
    "                4 * aligned_embed_dim * aligned_embed_dim\n",
    "            )\n",
    "            write_tensor_aligned(\n",
    "                fc2_bias,\n",
    "                f'layer_{layer_idx}_fc2_bias',\n",
    "                aligned_embed_dim\n",
    "            )\n",
    "            \n",
    "            # Skip mlp_output_offset, residual2_output_offset (runtime buffers)\n",
    "            # Skip layer_end_canary_offset (debug marker)\n",
    "        \n",
    "        # 5. Final LayerNorm\n",
    "        print(\"\\n🏁 Writing final LayerNorm...\")\n",
    "        write_tensor_aligned(\n",
    "            state_dict['ln_f.weight'],\n",
    "            'final_ln_weight',\n",
    "            aligned_embed_dim\n",
    "        )\n",
    "        write_tensor_aligned(\n",
    "            state_dict['ln_f.bias'],\n",
    "            'final_ln_bias',\n",
    "            aligned_embed_dim\n",
    "        )\n",
    "        \n",
    "        # Skip final_ln_mean_offset, final_ln_rstd_offset (runtime buffers)\n",
    "        # Skip final_output_offset, logits_offset (runtime buffers)\n",
    "        \n",
    "    # Now reopen to calculate checksum and write header\n",
    "    with open(output_file, 'r+b') as f:\n",
    "        # Calculate checksum of weight data\n",
    "        f.seek(header_size)\n",
    "        weight_data = f.read()\n",
    "        checksum = hashlib.sha256(weight_data).digest()\n",
    "        \n",
    "        # Write header at beginning\n",
    "        f.seek(0)\n",
    "        \n",
    "        # Magic and version\n",
    "        f.write(b'BUMPWGT2')  # 8 bytes\n",
    "        f.write(struct.pack('I', 2))  # version 2, 4 bytes\n",
    "        f.write(struct.pack('I', 0))  # model_type: 0=GPT2, 4 bytes\n",
    "        \n",
    "        # Hyperparameters (6 * 4 = 24 bytes)\n",
    "        f.write(struct.pack('I', n_layers))\n",
    "        f.write(struct.pack('I', vocab_size))\n",
    "        f.write(struct.pack('I', embed_dim))\n",
    "        f.write(struct.pack('I', context_len))\n",
    "        f.write(struct.pack('I', n_heads))\n",
    "        f.write(struct.pack('I', head_dim))\n",
    "        \n",
    "        # Aligned dimensions (3 * 8 = 24 bytes, using uint64)\n",
    "        f.write(struct.pack('Q', aligned_embed_dim))\n",
    "        f.write(struct.pack('Q', aligned_head_dim))\n",
    "        f.write(struct.pack('Q', aligned_context))\n",
    "        \n",
    "        # Checksum (32 bytes)\n",
    "        f.write(checksum)\n",
    "        \n",
    "        # Reserved (32 bytes) - for future use\n",
    "        f.write(b'\\x00' * 32)\n",
    "    \n",
    "    # Save weight map for debugging\n",
    "    with open(output_file + '.map.json', 'w') as f:\n",
    "        json.dump({\n",
    "            'header': {\n",
    "                'model': model_name,\n",
    "                'layers': n_layers,\n",
    "                'embed_dim': embed_dim,\n",
    "                'aligned_embed_dim': aligned_embed_dim,\n",
    "                'vocab_size': vocab_size,\n",
    "                'context_len': context_len,\n",
    "                'n_heads': n_heads,\n",
    "                'head_dim': head_dim,\n",
    "                'aligned_head_dim': aligned_head_dim\n",
    "            },\n",
    "            'weights': weight_map,\n",
    "            'total_bytes': current_offset,\n",
    "            'checksum': checksum.hex()\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✅ Success!\")\n",
    "    print(f\"  Output file: {output_file}\")\n",
    "    print(f\"  Total size: {current_offset / (1024**3):.2f} GB\")\n",
    "    print(f\"  Weight map: {output_file}.map.json\")\n",
    "    print(f\"  Checksum: {checksum.hex()[:16]}...\")\n",
    "    \n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5319b011-7f70-4d9c-8058-4366265ce5eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_bump_weights('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18940a4b-708d-4ae2-bb01-8a7136e596d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#if __name__ == '__main__':\n",
    "#    import argparse\n",
    "#    parser = argparse.ArgumentParser()\n",
    "#    parser.add_argument('--model', default='gpt2', coldpla\n",
    "#                       choices=['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'])\n",
    "#    parser.add_argument('--output', default=None)\n",
    "#    args = parser.parse_args()\n",
    "    \n",
    "#    output = args.output or f'{args.model}_bump.weights'\n",
    "#    create_bump_weights(args.model, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae659b87-5c99-46e6-a968-a4e5691b02db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def verify_bump_weights(weight_file='gpt2_bump.weights', map_file='gpt2_bump.weights.map.json'):\n",
    "    \"\"\"\n",
    "    Verify the bump weight file structure and compare with original model\n",
    "    \"\"\"\n",
    "    print(\"🔍 Verifying Bump Weight File\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load the map file\n",
    "    with open(map_file, 'r') as f:\n",
    "        weight_map = json.load(f)\n",
    "    \n",
    "    print(\"📊 File Structure:\")\n",
    "    print(f\"  Total size: {weight_map['total_bytes'] / (1024**3):.2f} GB\")\n",
    "    print(f\"  Checksum: {weight_map['checksum'][:16]}...\")\n",
    "    print(f\"  Model config:\")\n",
    "    for key, value in weight_map['header'].items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    # Read and verify header\n",
    "    with open(weight_file, 'rb') as f:\n",
    "        # Read header\n",
    "        magic = f.read(8)\n",
    "        version = struct.unpack('I', f.read(4))[0]\n",
    "        model_type = struct.unpack('I', f.read(4))[0]\n",
    "        \n",
    "        print(f\"\\n📦 Header Verification:\")\n",
    "        print(f\"  Magic: {magic.decode('ascii', errors='ignore')}\")\n",
    "        print(f\"  Version: {version}\")\n",
    "        print(f\"  Model type: {model_type} (0=GPT2)\")\n",
    "        \n",
    "        # Read hyperparameters\n",
    "        num_layers = struct.unpack('I', f.read(4))[0]\n",
    "        vocab_size = struct.unpack('I', f.read(4))[0]\n",
    "        embed_dim = struct.unpack('I', f.read(4))[0]\n",
    "        context_len = struct.unpack('I', f.read(4))[0]\n",
    "        num_heads = struct.unpack('I', f.read(4))[0]\n",
    "        head_dim = struct.unpack('I', f.read(4))[0]\n",
    "        \n",
    "        print(f\"\\n  Hyperparameters from header:\")\n",
    "        print(f\"    Layers: {num_layers}\")\n",
    "        print(f\"    Vocab: {vocab_size}\")\n",
    "        print(f\"    Embed dim: {embed_dim}\")\n",
    "        print(f\"    Context: {context_len}\")\n",
    "        print(f\"    Heads: {num_heads}\")\n",
    "        print(f\"    Head dim: {head_dim}\")\n",
    "        \n",
    "        # Sample some weights\n",
    "        f.seek(128)  # Skip to weights (header is 128 bytes)\n",
    "        \n",
    "        print(f\"\\n🔬 Weight Samples:\")\n",
    "        \n",
    "        # Read first few token embeddings\n",
    "        token_emb_sample = np.frombuffer(f.read(4 * 10), dtype=np.float32)\n",
    "        print(f\"  Token embeddings [0:10]: {token_emb_sample}\")\n",
    "        \n",
    "        # Check value ranges\n",
    "        f.seek(128)\n",
    "        chunk = np.frombuffer(f.read(4 * 10000), dtype=np.float32)\n",
    "        print(f\"  Value statistics (first 10k floats):\")\n",
    "        print(f\"    Min: {chunk.min():.6f}\")\n",
    "        print(f\"    Max: {chunk.max():.6f}\")\n",
    "        print(f\"    Mean: {chunk.mean():.6f}\")\n",
    "        print(f\"    Std: {chunk.std():.6f}\")\n",
    "    \n",
    "    # Compare with original model\n",
    "    print(f\"\\n🔄 Comparing with original GPT-2 model...\")\n",
    "    model = GPT2Model.from_pretrained('gpt2')\n",
    "    \n",
    "    # Check token embeddings match\n",
    "    with open(weight_file, 'rb') as f:\n",
    "        f.seek(128)  # Skip header\n",
    "        \n",
    "        # Read token embeddings\n",
    "        aligned_embed_dim = weight_map['header']['aligned_embed_dim']\n",
    "        vocab_size = weight_map['header']['vocab_size']\n",
    "        \n",
    "        file_token_emb = np.zeros((vocab_size, embed_dim), dtype=np.float32)\n",
    "        for v in range(vocab_size):\n",
    "            row = np.frombuffer(f.read(4 * aligned_embed_dim), dtype=np.float32)\n",
    "            file_token_emb[v] = row[:embed_dim]  # Take only actual dims, not padding\n",
    "    \n",
    "    model_token_emb = model.wte.weight.detach().numpy()\n",
    "    \n",
    "    diff = np.abs(file_token_emb - model_token_emb).max()\n",
    "    print(f\"  Max difference in token embeddings: {diff:.2e}\")\n",
    "    \n",
    "    if diff < 1e-5:\n",
    "        print(\"  ✅ Token embeddings match!\")\n",
    "    else:\n",
    "        print(\"  ⚠️  Token embeddings have differences\")\n",
    "    \n",
    "    print(\"\\n✅ Verification complete!\")\n",
    "    return weight_map\n",
    "\n",
    "def create_tokenizer_files(output_dir='.'):\n",
    "    \"\"\"\n",
    "    Create tokenizer files for C runtime\n",
    "    \"\"\"\n",
    "    print(\"\\n🔤 Creating Tokenizer Files for C Runtime\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    # Save vocabulary\n",
    "    vocab_file = f'{output_dir}/gpt2_vocab.txt'\n",
    "    with open(vocab_file, 'w', encoding='utf-8') as f:\n",
    "        for token_id in range(tokenizer.vocab_size):\n",
    "            token = tokenizer.convert_ids_to_tokens(token_id)\n",
    "            # Handle special characters\n",
    "            if token.startswith('Ġ'):\n",
    "                token = token[1:]  # Remove the Ġ prefix (indicates space)\n",
    "                f.write(f\"{token_id}\\t \\t{token}\\n\")  # Tab indicates space prefix\n",
    "            else:\n",
    "                f.write(f\"{token_id}\\t\\t{token}\\n\")\n",
    "    \n",
    "    print(f\"  Saved vocabulary to {vocab_file}\")\n",
    "    print(f\"  Vocab size: {tokenizer.vocab_size}\")\n",
    "    \n",
    "    # Save merges (BPE rules)\n",
    "    merges_file = f'{output_dir}/gpt2_merges.txt'\n",
    "    with open(merges_file, 'w', encoding='utf-8') as f:\n",
    "        # Get BPE merges from tokenizer\n",
    "        merges = tokenizer.byte_encoder\n",
    "        for merge in tokenizer.bpe_ranks.keys():\n",
    "            f.write(f\"{merge[0]} {merge[1]}\\n\")\n",
    "    \n",
    "    print(f\"  Saved BPE merges to {merges_file}\")\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "def tokenize_for_c(text, tokenizer=None, output_file='input_tokens.bin'):\n",
    "    \"\"\"\n",
    "    Tokenize text and save in format for C runtime\n",
    "    \"\"\"\n",
    "    if tokenizer is None:\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    print(f\"\\n📝 Tokenizing: '{text}'\")\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer.encode(text)\n",
    "    \n",
    "    print(f\"  Token IDs: {tokens}\")\n",
    "    print(f\"  Tokens: {[tokenizer.decode([t]) for t in tokens]}\")\n",
    "    print(f\"  Length: {len(tokens)} tokens\")\n",
    "    \n",
    "    # Save as binary file for C\n",
    "    with open(output_file, 'wb') as f:\n",
    "        # Write number of tokens\n",
    "        f.write(struct.pack('I', len(tokens)))\n",
    "        # Write token IDs as int32\n",
    "        for token_id in tokens:\n",
    "            f.write(struct.pack('i', token_id))\n",
    "    \n",
    "    print(f\"  Saved to {output_file}\")\n",
    "    \n",
    "    # Also create a C header file\n",
    "    header_file = output_file.replace('.bin', '.h')\n",
    "    with open(header_file, 'w') as f:\n",
    "        f.write(f\"// Auto-generated tokenization for: {text}\\n\")\n",
    "        f.write(f\"#define NUM_TOKENS {len(tokens)}\\n\")\n",
    "        f.write(f\"int32_t input_tokens[NUM_TOKENS] = {{\\n    \")\n",
    "        f.write(\", \".join(str(t) for t in tokens))\n",
    "        f.write(\"\\n};\\n\")\n",
    "    \n",
    "    print(f\"  C header saved to {header_file}\")\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def visualize_weight_layout(map_file='gpt2_bump.weights.map.json'):\n",
    "    \"\"\"\n",
    "    Visualize the memory layout of weights\n",
    "    \"\"\"\n",
    "    with open(map_file, 'r') as f:\n",
    "        weight_map = json.load(f)\n",
    "    \n",
    "    print(\"\\n📊 Weight Memory Layout Visualization\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    weights = weight_map['weights']\n",
    "    \n",
    "    # Group by layer\n",
    "    embeddings = {}\n",
    "    layers = {i: {} for i in range(12)}\n",
    "    final = {}\n",
    "    \n",
    "    for name, info in weights.items():\n",
    "        if 'layer_' in name:\n",
    "            layer_num = int(name.split('_')[1])\n",
    "            layers[layer_num][name] = info\n",
    "        elif 'final' in name:\n",
    "            final[name] = info\n",
    "        else:\n",
    "            embeddings[name] = info\n",
    "    \n",
    "    def format_size(bytes):\n",
    "        if bytes < 1024:\n",
    "            return f\"{bytes}B\"\n",
    "        elif bytes < 1024**2:\n",
    "            return f\"{bytes/1024:.1f}KB\"\n",
    "        elif bytes < 1024**3:\n",
    "            return f\"{bytes/1024**2:.1f}MB\"\n",
    "        else:\n",
    "            return f\"{bytes/1024**3:.2f}GB\"\n",
    "    \n",
    "    # Print layout\n",
    "    print(\"EMBEDDINGS:\")\n",
    "    for name, info in embeddings.items():\n",
    "        print(f\"  {name:30} @ 0x{info['offset']:08x} [{format_size(info['bytes']):>8}]\")\n",
    "    \n",
    "    print(\"\\nLAYERS:\")\n",
    "    for layer_num in range(12):\n",
    "        layer_size = sum(w['bytes'] for w in layers[layer_num].values())\n",
    "        print(f\"  Layer {layer_num:2d}: {format_size(layer_size):>8}\")\n",
    "        if layer_num == 0:  # Show details for first layer\n",
    "            for name, info in sorted(layers[layer_num].items(), key=lambda x: x[1]['offset']):\n",
    "                short_name = name.replace(f'layer_{layer_num}_', '')\n",
    "                print(f\"    {short_name:20} [{format_size(info['bytes']):>8}]\")\n",
    "    \n",
    "    print(\"\\nFINAL:\")\n",
    "    for name, info in final.items():\n",
    "        print(f\"  {name:30} @ 0x{info['offset']:08x} [{format_size(info['bytes']):>8}]\")\n",
    "    \n",
    "    # Memory map bar chart\n",
    "    print(\"\\n📊 Memory Usage by Component:\")\n",
    "    \n",
    "    emb_size = sum(w['bytes'] for w in embeddings.values())\n",
    "    layer_size = sum(sum(w['bytes'] for w in layers[i].values()) for i in range(12))\n",
    "    final_size = sum(w['bytes'] for w in final.values())\n",
    "    \n",
    "    total = emb_size + layer_size + final_size\n",
    "    \n",
    "    def bar(size, total, width=40):\n",
    "        n = int(size / total * width)\n",
    "        return '█' * n + '░' * (width - n)\n",
    "    \n",
    "    print(f\"  Embeddings: {bar(emb_size, total)} {format_size(emb_size):>8} ({emb_size/total*100:.1f}%)\")\n",
    "    print(f\"  Layers:     {bar(layer_size, total)} {format_size(layer_size):>8} ({layer_size/total*100:.1f}%)\")\n",
    "    print(f\"  Final:      {bar(final_size, total)} {format_size(final_size):>8} ({final_size/total*100:.1f}%)\")\n",
    "\n",
    "def test_simple_inference():\n",
    "    \"\"\"\n",
    "    Create a simple test case for C runtime\n",
    "    \"\"\"\n",
    "    print(\"\\n🧪 Creating Test Case for C Runtime\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test texts\n",
    "    test_texts = [\n",
    "        \"Hello world\",\n",
    "        \"The quick brown fox\",\n",
    "        \"Once upon a time\",\n",
    "        \"def hello():\",\n",
    "        \"import numpy as np\"\n",
    "    ]\n",
    "    \n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    for i, text in enumerate(test_texts):\n",
    "        print(f\"\\nTest {i+1}: '{text}'\")\n",
    "        tokens = tokenize_for_c(text, tokenizer, f'test_{i}.bin')\n",
    "        \n",
    "        # Also show what the expected output shape would be\n",
    "        print(f\"  Expected output shape: [{len(tokens)} x 768] -> [{len(tokens)} x 50257] logits\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Run all verification steps\n",
    "    weight_map = verify_bump_weights()\n",
    "    visualize_weight_layout()\n",
    "    \n",
    "    # Create tokenizer files\n",
    "    tokenizer = create_tokenizer_files()\n",
    "    \n",
    "    # Create test cases\n",
    "    test_simple_inference()\n",
    "    \n",
    "    # Example: tokenize custom text\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"💡 Example: Tokenize your own text\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    custom_text = \"The artificial intelligence model\"\n",
    "    tokens = tokenize_for_c(custom_text, tokenizer, 'custom_input.bin')\n",
    "    \n",
    "    print(\"\\n✅ All verification and preprocessing complete!\")\n",
    "    print(\"\\nTo use in C:\")\n",
    "    print(\"  1. Load weights: gpt2_bump.weights\")\n",
    "    print(\"  2. Load tokens: custom_input.bin\")\n",
    "    print(\"  3. Run inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9618c6e1-709b-4b50-a22c-1fb287b116c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab93eab-f53f-4da8-861a-a9cd074d2a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fb15fa-96e6-4d26-8c6a-69c0b0ddc8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python with transformers library:\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "tokens = [1, 2, 3, 42225, 19820, 39356, 40127, 45816, 9928, 16847, 38608, 13960, 27840]\n",
    "text = tokenizer.decode(tokens)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31458939-3d6a-4175-931f-06f72bd901eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
