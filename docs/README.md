# C-Transformer Documentation

**Cache-Optimized Transformer Training Engine in Pure C**

Welcome to the comprehensive documentation for C-Transformer, a CPU-first transformer implementation designed for training large language models on commodity hardware.

---

## üìö Documentation Index

### Getting Started

1. **[Usage Guide](USAGE_GUIDE.md)** - **START HERE**
   - Quick start tutorial
   - Compilation instructions
   - Training from scratch
   - Checkpoint management
   - Command-line reference
   - Troubleshooting

### Deep Dives

2. **[Backpropagation Flow](BACKPROP_FLOW.md)**
   - Complete step-by-step backward pass walkthrough
   - Layer-by-layer gradient flow
   - Memory layout and data structures
   - Implementation patterns

3. **[Numerical Methods & Mathematics](NUMERICAL_METHODS.md)** - **RECOMMENDED**
   - Full mathematical derivations for all operations
   - Softmax Jacobian derivation (with proof!)
   - Cross-entropy loss gradients
   - LayerNorm backward pass derivation
   - GELU activation mathematics
   - Attention mechanism gradients
   - **All numerical stability tricks explained**:
     - Log-sum-exp for softmax
     - Epsilon for division by zero
     - Gradient accumulation patterns

### Comparisons

4. **[Comparison with gemma.cpp](COMPARISON_WITH_GEMMA_CPP.md)**
   - How C-Transformer compares to Google's gemma.cpp
   - SIMD strategy differences (direct AVX-512 vs Highway)
   - Memory layout comparison
   - ARM porting difficulty analysis
   - Strategic positioning

---

## üéØ Quick Navigation

### I want to...

**Train my first model**
‚Üí [Usage Guide: Quick Start](USAGE_GUIDE.md#quick-start)

**Understand the math behind backprop**
‚Üí [Numerical Methods: Softmax Jacobian](NUMERICAL_METHODS.md#softmax-backward-the-jacobian-derivation)

**Learn about numerical stability tricks**
‚Üí [Numerical Methods: Stability Techniques](NUMERICAL_METHODS.md#numerical-stability-techniques)

**See the complete backward pass flow**
‚Üí [Backprop Flow: Layer-by-Layer](BACKPROP_FLOW.md#layer-by-layer-backward-flow)

**Compare with Google's implementation**
‚Üí [Comparison: C-Transformer vs gemma.cpp](COMPARISON_WITH_GEMMA_CPP.md)

**Debug training issues**
‚Üí [Usage Guide: Troubleshooting](USAGE_GUIDE.md#troubleshooting)

---

## üî¨ Technical Highlights

### Architecture

- **Pure C implementation** - No external dependencies except standard library + OpenMP
- **Single memory allocation** - Bump allocator with hugepage backing
- **Training support** - Full backpropagation with gradient accumulation
- **CPU-optimized** - AVX-512 vectorization, cache-aware blocking

### Memory Layout

```
[Token Embeddings][Position Embeddings][Layer 0 Weights]...[Layer N Weights]
[Forward Activations][Backward Gradients][Training Buffers]
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Single Contiguous Block ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

Total: ~1.17 GB for L=4, d=256 model (0.37 GB forward, 0.80 GB gradients)

### Optimization Techniques

All documented in [Numerical Methods](NUMERICAL_METHODS.md):

1. **Log-sum-exp trick** - Prevents overflow/underflow in softmax
2. **Cache-blocked GEMM** - Maximizes L1/L2 cache hits
3. **Token-parallel computation** - Leverages multi-core CPUs
4. **Hugepage backing** - Reduces TLB misses
5. **In-place gradient updates** - Minimizes memory bandwidth

---

## üìñ Documentation Features

### Mathematical Rigor

Every backward pass operation includes:
- ‚úÖ **Full derivation** - Step-by-step chain rule application
- ‚úÖ **Matrix shapes** - Dimensions verified at each step
- ‚úÖ **Numerical stability** - Overflow/underflow prevention explained
- ‚úÖ **Implementation code** - Linked to actual source locations

**Example: Softmax Backward**

The documentation shows:
1. Forward pass formula
2. Jacobian matrix derivation (diagonal vs off-diagonal)
3. Final simplified formula: `dx = y ‚äô (dy - dot(y, dy))`
4. Implementation code (main.c:5791-5829)
5. Why it works (constraint preservation)

### Code Cross-References

All mathematical explanations reference actual implementation:

- `main.c:3154` - Log-sum-exp trick in forward softmax
- `main.c:5791` - Softmax Jacobian in backward pass
- `main.c:6231` - Cross-entropy gradient computation
- `main.c:5496` - LayerNorm backward derivation

### Visual Aids

Includes ASCII diagrams for:
- Memory layout
- Gradient flow direction
- Training loop structure
- Attention computation phases

---

## üöÄ Performance Characteristics

### Model: L=4, d=256, ctx=1024

- **Memory**: 1.17 GB total
- **Training speed**: ~30 tokens/sec (28-core Xeon)
- **FLOPs per step**: ~240M (80M forward + 160M backward)
- **Cores utilized**: 28 of 32 (reserve 4 for OS)

### Scaling

Model size vs memory requirements:

| Layers | d_model | Parameters | Memory (train) |
|--------|---------|------------|----------------|
| 4 | 256 | ~12M | 1.2 GB |
| 8 | 512 | ~85M | 4.5 GB |
| 12 | 768 | ~300M | 12 GB |
| 24 | 1024 | ~1.2B | 40 GB |

---

## üéì Educational Value

This documentation is designed for:

1. **Learning transformer internals** - See exactly how backprop works
2. **Understanding CPU optimization** - Cache-aware design patterns
3. **Numerical stability** - Why and how we prevent overflow/underflow
4. **Systems programming** - Memory management, threading, SIMD

**Pedagogical approach**:
- Start with math (why it works)
- Show implementation (how it's coded)
- Explain optimizations (why this way is faster)

---

## üîó External Resources

### Papers Referenced

1. **Attention Is All You Need** (Vaswani et al., 2017)
   - Original transformer architecture

2. **Layer Normalization** (Ba et al., 2016)
   - LayerNorm backward pass derivation

3. **GELU** (Hendrycks & Gimpel, 2016)
   - Gaussian Error Linear Units activation

4. **GPT-2** (Radford et al., 2019)
   - Language model architecture

### Recommended Reading Order

1. [Usage Guide](USAGE_GUIDE.md) - Get it running
2. [Backprop Flow](BACKPROP_FLOW.md) - Understand the code structure
3. [Numerical Methods](NUMERICAL_METHODS.md) - Deep dive into mathematics
4. [Comparison](COMPARISON_WITH_GEMMA_CPP.md) - See how it compares to Google

---

## üõ†Ô∏è Developer Tools

### Generate HTML Documentation

```bash
# Install Doxygen (Ubuntu/Debian)
sudo apt-get install doxygen graphviz

# Generate documentation
cd /path/to/C-Transformer
doxygen Doxyfile

# Open in browser
firefox docs/html/index.html
```

### Documentation Structure

```
docs/
‚îú‚îÄ‚îÄ README.md                    (this file)
‚îú‚îÄ‚îÄ USAGE_GUIDE.md              (how to run)
‚îú‚îÄ‚îÄ BACKPROP_FLOW.md            (implementation flow)
‚îú‚îÄ‚îÄ NUMERICAL_METHODS.md        (mathematics)
‚îú‚îÄ‚îÄ COMPARISON_WITH_GEMMA_CPP.md (Google comparison)
‚îú‚îÄ‚îÄ Doxyfile                    (Doxygen config)
‚îî‚îÄ‚îÄ html/                       (generated HTML docs)
    ‚îî‚îÄ‚îÄ index.html
```

---

## ü§ù Contributing

### Improving Documentation

Found an error or unclear explanation?

1. Check if the issue is in:
   - Mathematical derivation ‚Üí [NUMERICAL_METHODS.md](NUMERICAL_METHODS.md)
   - Code flow explanation ‚Üí [BACKPROP_FLOW.md](BACKPROP_FLOW.md)
   - Usage instructions ‚Üí [USAGE_GUIDE.md](USAGE_GUIDE.md)

2. Submit issue at: https://github.com/antshiv/C-Transformer/issues

### Adding New Sections

Want to add documentation for:
- New optimization techniques
- Additional architectures (LLAMA, etc.)
- ARM/NEON port guide

Follow the existing style:
1. Mathematical derivation first
2. Implementation code second
3. Optimization rationale third

---

## üìä Documentation Statistics

- **Total markdown files**: 5
- **Mathematical derivations**: 7 (softmax, cross-entropy, LayerNorm, GELU, etc.)
- **Code cross-references**: 50+
- **Diagrams**: 10+ ASCII diagrams
- **Examples**: 20+ command-line examples

---

## üìù Version

**Documentation version**: 1.0
**Code version**: Compatible with C-Transformer as of 2025-11-07
**Last updated**: 2025-11-07

---

## üìß Contact

**Project**: ANTSHIV ROBOTICS
**Author**: Antshiv
**Purpose**: Conservation AI - Embedded intelligence for biodiversity monitoring

See main project README for conservation robotics context.

---

*Generated with Claude Code - Comprehensive C-Transformer documentation*
