# ğŸš€ C-Transformers: Cache-Optimized Transformers in C

This project is a pure C implementation of a GPT-style transformer model with:
- ğŸ§  Fully contiguous, single-block memory layout
- ğŸ“ 64-byte alignment for every tensor
- ğŸ§± Hugepage-backed allocations (2MB) to minimize TLB misses
- âš™ï¸ Inline memory bump allocator to track offsets precisely
- ğŸ”§ Dry-run and allocation modes for profiling model memory capacity

---

## ğŸ“º YouTube Series

This repo tracks my video series on building high-performance embedded AI systems.

â–¶ï¸ **Series Playlist**: [Antshiv Robotics YouTube](https://www.youtube.com/@AntshivRobotics)  
ğŸ§µ Each video will have its own tagged release (`v1.0`, `v1.1`, etc.) that matches the code exactly.

---

## ğŸ§  Features

- Optimized layout for GPT-2-style transformers
- Clean C code with no external dependencies
- Command-line options to configure model dimensions:
  - `--layers`
  - `--dmodel`
  - `--ctx`
  - `--vocab`
- `--force` option to trigger actual memory allocation

---

## ğŸ§ª Example

```bash
# Dry run (estimate memory usage only)
./main --layers 12 --dmodel 384 --ctx 256 --vocab 32768

# Force allocation with hugepages
./main --layers 12 --dmodel 384 --ctx 256 --vocab 32768 --force
