{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WikiSQL Training Data Preparation\n",
    "\n",
    "This notebook prepares training data for SQL generation using the WikiSQL dataset.\n",
    "\n",
    "**Dataset:** [WikiSQL](https://huggingface.co/datasets/wikisql) - 80k+ examples of natural language questions → SQL queries\n",
    "\n",
    "**Format:** `[TASK: SQL] Table: <schema> | Question: <question> → SELECT query`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets tiktoken pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many training examples to use\n",
    "NUM_SAMPLES = 10000  # Start with 10k, can increase to 20k or 80k\n",
    "\n",
    "# Context window settings\n",
    "CONTEXT_LENGTH = 1024\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"./data/sql_training_pairs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load WikiSQL Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "print(\"Loading WikiSQL dataset...\")\n",
    "ds = load_dataset(\"wikisql\")\n",
    "\n",
    "print(f\"\\nDataset splits:\")\n",
    "print(f\"  Train: {len(ds['train'])} examples\")\n",
    "print(f\"  Validation: {len(ds['validation'])} examples\")\n",
    "print(f\"  Test: {len(ds['test'])} examples\")\n",
    "\n",
    "# Convert to pandas for easier manipulation\n",
    "df = ds['train'].to_pandas()\n",
    "print(f\"\\nUsing first {NUM_SAMPLES} training examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore Dataset Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at first example\n",
    "example = df.iloc[0]\n",
    "\n",
    "print(\"Example WikiSQL entry:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Question: {example['question']}\")\n",
    "print(f\"\\nTable:\")\n",
    "print(f\"  Columns: {example['table']['header']}\")\n",
    "print(f\"  Column Types: {example['table']['types']}\")\n",
    "print(f\"\\nSQL Query Components:\")\n",
    "print(f\"  Aggregation: {example['sql']['agg']}\")\n",
    "print(f\"  Select Column: {example['sql']['sel']}\")\n",
    "print(f\"  Conditions: {example['sql']['conds']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert WikiSQL Format to SQL String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikisql_to_sql_string(sql_dict, table_header):\n",
    "    \"\"\"\n",
    "    Convert WikiSQL's structured SQL format to SQL string.\n",
    "    \n",
    "    sql_dict contains:\n",
    "      - agg: aggregation function (0=none, 1=MAX, 2=MIN, 3=COUNT, 4=SUM, 5=AVG)\n",
    "      - sel: column index to select\n",
    "      - conds: list of [column_idx, operator, value]\n",
    "    \"\"\"\n",
    "    agg_ops = ['', 'MAX', 'MIN', 'COUNT', 'SUM', 'AVG']\n",
    "    cond_ops = ['=', '>', '<', 'OP']\n",
    "    \n",
    "    # Build SELECT clause\n",
    "    agg_idx = sql_dict['agg']\n",
    "    sel_col = table_header[sql_dict['sel']]\n",
    "    \n",
    "    if agg_idx == 0:\n",
    "        select_clause = f\"SELECT {sel_col}\"\n",
    "    else:\n",
    "        select_clause = f\"SELECT {agg_ops[agg_idx]}({sel_col})\"\n",
    "    \n",
    "    # Build FROM clause (WikiSQL doesn't provide table names, use generic)\n",
    "    from_clause = \"FROM table\"\n",
    "    \n",
    "    # Build WHERE clause\n",
    "    if len(sql_dict['conds']) == 0:\n",
    "        return f\"{select_clause} {from_clause}\"\n",
    "    \n",
    "    conditions = []\n",
    "    for col_idx, op_idx, value in sql_dict['conds']:\n",
    "        col_name = table_header[col_idx]\n",
    "        operator = cond_ops[op_idx] if op_idx < len(cond_ops) else '='\n",
    "        \n",
    "        # Add quotes if value is string\n",
    "        if isinstance(value, str):\n",
    "            value = f\"'{value}'\"\n",
    "        \n",
    "        conditions.append(f\"{col_name} {operator} {value}\")\n",
    "    \n",
    "    where_clause = \"WHERE \" + \" AND \".join(conditions)\n",
    "    \n",
    "    return f\"{select_clause} {from_clause} {where_clause}\"\n",
    "\n",
    "# Test with first example\n",
    "example = df.iloc[0]\n",
    "sql_string = wikisql_to_sql_string(example['sql'], example['table']['header'])\n",
    "\n",
    "print(\"Test SQL conversion:\")\n",
    "print(f\"Question: {example['question']}\")\n",
    "print(f\"Generated SQL: {sql_string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Format Training Examples with Task Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sql_example(row):\n",
    "    \"\"\"\n",
    "    Format: [TASK: SQL] Table: col1, col2, col3 | Question: what is x? → SELECT query\n",
    "    \"\"\"\n",
    "    # Get table schema\n",
    "    columns = \", \".join(row['table']['header'])\n",
    "    \n",
    "    # Get question\n",
    "    question = row['question']\n",
    "    \n",
    "    # Convert SQL to string\n",
    "    sql_query = wikisql_to_sql_string(row['sql'], row['table']['header'])\n",
    "    \n",
    "    # Format with task prefix\n",
    "    prompt = f\"[TASK: SQL] Table: {columns} | Question: {question}\"\n",
    "    completion = sql_query\n",
    "    \n",
    "    # Concatenate input + output for next-token prediction\n",
    "    full_text = f\"{prompt} → {completion}\"\n",
    "    \n",
    "    return full_text\n",
    "\n",
    "# Test formatting\n",
    "example_text = format_sql_example(df.iloc[0])\n",
    "print(\"Formatted training example:\")\n",
    "print(\"=\"*60)\n",
    "print(example_text)\n",
    "print(\"=\"*60)\n",
    "print(f\"Length: {len(example_text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Training Pairs and Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Format all examples and tokenize\n",
    "print(f\"Processing {NUM_SAMPLES} examples...\")\n",
    "\n",
    "training_examples = []\n",
    "skipped = 0\n",
    "\n",
    "for idx in range(min(NUM_SAMPLES, len(df))):\n",
    "    row = df.iloc[idx]\n",
    "    \n",
    "    # Format example\n",
    "    formatted_text = format_sql_example(row)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer.encode(formatted_text)\n",
    "    \n",
    "    # Skip if too long (need room for input + target)\n",
    "    if len(tokens) > CONTEXT_LENGTH:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    \n",
    "    training_examples.append({\n",
    "        'text': formatted_text,\n",
    "        'tokens': tokens,\n",
    "        'ctx_len': len(tokens),  # Actual length (variable)\n",
    "        'example_id': idx\n",
    "    })\n",
    "    \n",
    "    if (idx + 1) % 1000 == 0:\n",
    "        print(f\"  Processed {idx + 1}/{NUM_SAMPLES}\")\n",
    "\n",
    "print(f\"\\nCreated {len(training_examples)} training examples\")\n",
    "print(f\"Skipped {skipped} examples (too long)\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_train = pd.DataFrame(training_examples)\n",
    "\n",
    "# Show statistics\n",
    "print(f\"\\nToken length statistics:\")\n",
    "print(df_train['ctx_len'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Inspect Training Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few examples\n",
    "print(\"Sample Training Examples:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(min(5, len(df_train))):\n",
    "    example = df_train.iloc[i]\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"  Text: {example['text']}\")\n",
    "    print(f\"  Tokens: {example['ctx_len']}\")\n",
    "    print(f\"  First 10 token IDs: {example['tokens'][:10]}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save as Binary Files for C-Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import struct\nfrom pathlib import Path\n\n# Create output directory\noutput_dir = Path(OUTPUT_DIR)\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nprint(f\"Saving {len(df_train)} training pairs to {output_dir}\")\nprint(\"=\"*60)\n\nfor idx, row in df_train.iterrows():\n    tokens = row['tokens']\n    \n    # Clamp to max context length BEFORE writing header (defensive programming)\n    # Step 5 already filters out long examples, but this ensures header always matches file\n    ctx_len = min(len(tokens), CONTEXT_LENGTH)\n    \n    # Create sequence: ctx_len input tokens + 1 padding token for next-token target\n    # C code uses: input = tokens[0:ctx_len], target = tokens[1:ctx_len+1]\n    sequence = tokens[:ctx_len] + [0]  # Always exactly ctx_len + 1 tokens\n    \n    # File format: [uint16 ctx_len][uint16 tgt_len][tokens...]\n    pair_file = output_dir / f\"pair_{idx:05d}.bin\"\n    \n    with open(pair_file, 'wb') as f:\n        # Write header: ctx_len and target_len (always 1 for next-token prediction)\n        f.write(struct.pack('HH', ctx_len, 1))\n        \n        # Write exactly ctx_len + 1 tokens (matches header)\n        f.write(struct.pack(f'{len(sequence)}I', *sequence))\n    \n    if (idx + 1) % 1000 == 0:\n        print(f\"  Saved {idx + 1}/{len(df_train)}\")\n\nprint(f\"\\n✓ Complete! Saved {len(df_train)} binary files\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Verify Binary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back first file to verify\n",
    "test_file = output_dir / \"pair_00000.bin\"\n",
    "\n",
    "with open(test_file, 'rb') as f:\n",
    "    # Read header\n",
    "    header = f.read(4)\n",
    "    ctx_len, tgt_len = struct.unpack('HH', header)\n",
    "    \n",
    "    # Read tokens\n",
    "    total_tokens = ctx_len + tgt_len\n",
    "    tokens_bytes = f.read(4 * total_tokens)\n",
    "    tokens = list(struct.unpack(f'{total_tokens}I', tokens_bytes))\n",
    "\n",
    "print(f\"Verification of {test_file.name}:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Header:\")\n",
    "print(f\"  ctx_len: {ctx_len}\")\n",
    "print(f\"  tgt_len: {tgt_len}\")\n",
    "print(f\"\\nTokens: {len(tokens)} total\")\n",
    "print(f\"  First 10: {tokens[:10]}\")\n",
    "print(f\"\\nDecoded text:\")\n",
    "print(f\"  {tokenizer.decode(tokens[:ctx_len])}\")\n",
    "print(f\"\\nMatches original: {tokens[:ctx_len] == df_train.iloc[0]['tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "metadata = {\n",
    "    'dataset': 'WikiSQL',\n",
    "    'task': 'SQL generation',\n",
    "    'num_examples': len(df_train),\n",
    "    'context_length': CONTEXT_LENGTH,\n",
    "    'vocab_size': 50257,\n",
    "    'tokenizer': 'gpt2',\n",
    "    'format': '[TASK: SQL] Table: columns | Question: question → SQL query',\n",
    "    'avg_tokens': float(df_train['ctx_len'].mean()),\n",
    "    'max_tokens': int(df_train['ctx_len'].max()),\n",
    "    'min_tokens': int(df_train['ctx_len'].min())\n",
    "}\n",
    "\n",
    "metadata_file = output_dir / \"metadata.json\"\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Saved metadata:\")\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SQL TRAINING DATA READY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset:           WikiSQL\")\n",
    "print(f\"Training examples: {len(df_train)}\")\n",
    "print(f\"Context length:    {CONTEXT_LENGTH}\")\n",
    "print(f\"Avg tokens/sample: {df_train['ctx_len'].mean():.1f}\")\n",
    "print(f\"Files location:    {output_dir}\")\n",
    "print()\n",
    "print(\"File format:\")\n",
    "print(\"  [uint16 ctx_len][uint16 tgt_len][ctx_len+tgt_len uint32 tokens]\")\n",
    "print()\n",
    "print(\"C-Transformer will:\")\n",
    "print(\"  1. Read ctx_len from header\")\n",
    "print(\"  2. Load ctx_len + 1 tokens\")\n",
    "print(\"  3. input = tokens[0:ctx_len]\")\n",
    "print(\"  4. target = tokens[1:ctx_len+1] (next-token prediction)\")\n",
    "print(\"  5. Compute loss only over ctx_len positions\")\n",
    "print()\n",
    "print(f\"Expected training time: ~{len(df_train) * 10 // 3600}-{len(df_train) * 20 // 3600} hours\")\n",
    "print(f\"  (depends on CPU and batch size)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}