{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WikiSQL Training Data Preparation\n",
    "\n",
    "This notebook prepares training data for SQL generation using the WikiSQL dataset.\n",
    "\n",
    "**Dataset:** [WikiSQL](https://huggingface.co/datasets/wikisql) - 80k+ examples of natural language questions \u2192 SQL queries\n",
    "\n",
    "**Format:** `[TASK: SQL] Table: <schema> | Question: <question> \u2192 SELECT query`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets tiktoken pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many training examples to use\n",
    "NUM_SAMPLES = 10000  # Start with 10k, can increase to 20k or 80k\n",
    "\n",
    "# Context window settings\n",
    "CONTEXT_LENGTH = 1024\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"./data/sql_training_pairs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load WikiSQL Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset\nimport pandas as pd\nimport tiktoken\n\nprint(\"Loading WikiSQL dataset...\")\n# Use htriedman/wikisql - already formatted as input/output pairs\nds = load_dataset(\"htriedman/wikisql\")\n\nprint(f\"\\nDataset splits:\")\nprint(f\"  Train: {len(ds['train'])} examples\")\nprint(f\"  Validation: {len(ds['validation'])} examples\")\nprint(f\"  Test: {len(ds['test'])} examples\")\n\n# Convert to pandas for easier manipulation\ndf = ds['train'].to_pandas()\nprint(f\"\\nUsing first {NUM_SAMPLES} training examples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore Dataset Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Look at first example\nexample = df.iloc[0]\n\nprint(\"Example WikiSQL entry:\")\nprint(\"=\"*60)\nprint(f\"Instruction: {example['instruction']}\")\nprint(f\"Input (question): {example['input']}\")\nprint(f\"Output (SQL): {example['output']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## Step 3: Format Training Examples with Task Prefix"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def format_sql_example(row):\n    \"\"\"\n    Format: [TASK: SQL] Question: what is x? \u2192 SQL query\n    \n    htriedman/wikisql already has:\n      - instruction: \"Translate the following into a SQL query\"\n      - input: natural language question\n      - output: SQL query\n    \"\"\"\n    question = row['input']\n    sql_query = row['output']\n    \n    # Format with task prefix\n    prompt = f\"[TASK: SQL] Question: {question}\"\n    completion = sql_query\n    \n    # Concatenate input + output for next-token prediction\n    full_text = f\"{prompt} \u2192 {completion}\"\n    \n    return full_text\n\n# Test formatting\nexample_text = format_sql_example(df.iloc[0])\nprint(\"Formatted training example:\")\nprint(\"=\"*60)\nprint(example_text)\nprint(\"=\"*60)\nprint(f\"Length: {len(example_text)} characters\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## Step 4: Create Training Pairs and Tokenize"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "<cell_type>markdown</cell_type>## Step 5: Inspect Training Examples"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## Step 6: Save as Binary Files for C-Transformer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Format all examples and tokenize\n",
    "print(f\"Processing {NUM_SAMPLES} examples...\")\n",
    "\n",
    "training_examples = []\n",
    "skipped = 0\n",
    "\n",
    "for idx in range(min(NUM_SAMPLES, len(df))):\n",
    "    row = df.iloc[idx]\n",
    "    \n",
    "    # Format example\n",
    "    formatted_text = format_sql_example(row)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer.encode(formatted_text)\n",
    "    \n",
    "    # Skip if too long (need room for input + target)\n",
    "    if len(tokens) > CONTEXT_LENGTH:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    \n",
    "    training_examples.append({\n",
    "        'text': formatted_text,\n",
    "        'tokens': tokens,\n",
    "        'ctx_len': len(tokens),  # Actual length (variable)\n",
    "        'example_id': idx\n",
    "    })\n",
    "    \n",
    "    if (idx + 1) % 1000 == 0:\n",
    "        print(f\"  Processed {idx + 1}/{NUM_SAMPLES}\")\n",
    "\n",
    "print(f\"\\nCreated {len(training_examples)} training examples\")\n",
    "print(f\"Skipped {skipped} examples (too long)\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_train = pd.DataFrame(training_examples)\n",
    "\n",
    "# Show statistics\n",
    "print(f\"\\nToken length statistics:\")\n",
    "print(df_train['ctx_len'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## Step 7: Verify Binary Files"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few examples\n",
    "print(\"Sample Training Examples:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(min(5, len(df_train))):\n",
    "    example = df_train.iloc[i]\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"  Text: {example['text']}\")\n",
    "    print(f\"  Tokens: {example['ctx_len']}\")\n",
    "    print(f\"  First 10 token IDs: {example['tokens'][:10]}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## Step 8: Save Metadata"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(OUTPUT_DIR)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Saving {len(df_train)} training pairs to {output_dir}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for idx, row in df_train.iterrows():\n",
    "    tokens = row['tokens']\n",
    "    total_tokens = len(tokens)\n",
    "\n",
    "    # Need at least two tokens to form an input/target pair\n",
    "    if total_tokens < 2:\n",
    "        continue\n",
    "\n",
    "    # ctx_len counts how many input positions we keep (reserve one for the next-token target)\n",
    "    ctx_len = min(total_tokens - 1, CONTEXT_LENGTH)\n",
    "\n",
    "    # Sequence stores ctx_len inputs plus the immediate next token for supervision\n",
    "    sequence = tokens[:ctx_len + 1]\n",
    "\n",
    "    # If the example was shorter than CONTEXT_LENGTH, pad the trailing target slot with 0\n",
    "    if len(sequence) < ctx_len + 1:\n",
    "        sequence = sequence + [0]\n",
    "\n",
    "    pair_file = output_dir / f\"pair_{idx:05d}.bin\"\n",
    "\n",
    "    with open(pair_file, 'wb') as f:\n",
    "        f.write(struct.pack('HH', ctx_len, 1))\n",
    "        f.write(struct.pack(f'{len(sequence)}I', *sequence))\n",
    "\n",
    "    if (idx + 1) % 1000 == 0:\n",
    "        print(f\"  Saved {idx + 1}/{len(df_train)}\")\n",
    "\n",
    "print(f\"\n",
    "\u2713 Complete! Saved {len(df_train)} binary files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Verify Binary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back first file to verify\n",
    "test_file = output_dir / \"pair_00000.bin\"\n",
    "\n",
    "with open(test_file, 'rb') as f:\n",
    "    # Read header\n",
    "    header = f.read(4)\n",
    "    ctx_len, tgt_len = struct.unpack('HH', header)\n",
    "    \n",
    "    # Read tokens\n",
    "    total_tokens = ctx_len + tgt_len\n",
    "    tokens_bytes = f.read(4 * total_tokens)\n",
    "    tokens = list(struct.unpack(f'{total_tokens}I', tokens_bytes))\n",
    "\n",
    "print(f\"Verification of {test_file.name}:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Header:\")\n",
    "print(f\"  ctx_len: {ctx_len}\")\n",
    "print(f\"  tgt_len: {tgt_len}\")\n",
    "print(f\"\\nTokens: {len(tokens)} total\")\n",
    "print(f\"  First 10: {tokens[:10]}\")\n",
    "print(f\"\\nDecoded text:\")\n",
    "print(f\"  {tokenizer.decode(tokens[:ctx_len])}\")\n",
    "print(f\"\\nMatches original: {tokens[:ctx_len] == df_train.iloc[0]['tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "metadata = {\n",
    "    'dataset': 'WikiSQL',\n",
    "    'task': 'SQL generation',\n",
    "    'num_examples': len(df_train),\n",
    "    'context_length': CONTEXT_LENGTH,\n",
    "    'vocab_size': 50257,\n",
    "    'tokenizer': 'gpt2',\n",
    "    'format': '[TASK: SQL] Table: columns | Question: question \u2192 SQL query',\n",
    "    'avg_tokens': float(df_train['ctx_len'].mean()),\n",
    "    'max_tokens': int(df_train['ctx_len'].max()),\n",
    "    'min_tokens': int(df_train['ctx_len'].min())\n",
    "}\n",
    "\n",
    "metadata_file = output_dir / \"metadata.json\"\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Saved metadata:\")\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SQL TRAINING DATA READY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset:           WikiSQL\")\n",
    "print(f\"Training examples: {len(df_train)}\")\n",
    "print(f\"Context length:    {CONTEXT_LENGTH}\")\n",
    "print(f\"Avg tokens/sample: {df_train['ctx_len'].mean():.1f}\")\n",
    "print(f\"Files location:    {output_dir}\")\n",
    "print()\n",
    "print(\"File format:\")\n",
    "print(\"  [uint16 ctx_len][uint16 tgt_len][ctx_len+tgt_len uint32 tokens]\")\n",
    "print()\n",
    "print(\"C-Transformer will:\")\n",
    "print(\"  1. Read ctx_len from header\")\n",
    "print(\"  2. Load ctx_len + 1 tokens\")\n",
    "print(\"  3. input = tokens[0:ctx_len]\")\n",
    "print(\"  4. target = tokens[1:ctx_len+1] (next-token prediction)\")\n",
    "print(\"  5. Compute loss only over ctx_len positions\")\n",
    "print()\n",
    "print(f\"Expected training time: ~{len(df_train) * 10 // 3600}-{len(df_train) * 20 // 3600} hours\")\n",
    "print(f\"  (depends on CPU and batch size)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}