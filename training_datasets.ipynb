{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51e9205-e5b0-4047-a20d-b16ebd7ceae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75740c7-a47e-4cd0-b84d-9c21c7d8854f",
   "metadata": {},
   "source": [
    "# GPT-2 Training Data Preparation\n",
    "\n",
    "Dataset: [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories)\n",
    "\n",
    "This notebook prepares training data for GPT-2 pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44b373e8-88b0-4756-ab2a-9e9fdb74b68b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One day, a little girl named Lily found a need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there was a little car named...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One day, a little fish named Fin was swimming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, in a land full of trees, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there was a little girl name...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  One day, a little girl named Lily found a need...\n",
       "1  Once upon a time, there was a little car named...\n",
       "2  One day, a little fish named Fin was swimming ...\n",
       "3  Once upon a time, in a land full of trees, the...\n",
       "4  Once upon a time, there was a little girl name..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notebook cell\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"roneneldan/TinyStories\")\n",
    "df = ds['train'].to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f58fe8-0cc8-4cf7-9dba-9c6bba0759dd",
   "metadata": {},
   "source": [
    "#### ============ STEP 1: Single Story Memorization Test ============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44340f32-43d0-441f-8374-49cf35717a15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test story:\n",
      "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n",
      "\n",
      "Length: 701 characters\n"
     ]
    }
   ],
   "source": [
    "test_story = df.iloc[0]['text']\n",
    "print(\"Test story:\")\n",
    "print(test_story)\n",
    "print(f\"\\nLength: {len(test_story)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef629189-0cff-4993-bfda-4b76f07e10ec",
   "metadata": {},
   "source": [
    "##### ============ STEP 2: Tokenize with GPT-2 ============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba262b99-3787-487d-a8c1-4c71b2f506d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 162\n",
      "First 20 tokens: [3198, 1110, 11, 257, 1310, 2576, 3706, 20037, 1043, 257, 17598, 287, 607, 2119, 13, 1375, 2993, 340, 373, 2408, 284, 711, 351, 340, 780, 340, 373, 7786, 13, 20037, 2227, 284, 2648, 262, 17598, 351, 607, 1995, 11, 523, 673, 714, 34249, 257, 4936, 319, 607, 10147, 13, 198, 198, 43, 813, 1816, 284, 607, 1995, 290, 531, 11, 366, 29252, 11, 314, 1043, 428, 17598, 13, 1680, 345, 2648, 340, 351, 502, 290, 34249, 616, 10147, 1701, 2332, 1995, 13541, 290, 531, 11, 366, 5297, 11, 20037, 11, 356, 460, 2648, 262, 17598, 290, 4259, 534, 10147, 526, 198, 198, 41631, 11, 484, 4888, 262, 17598, 290, 384, 19103, 262, 4936, 319, 20037, 338, 10147, 13, 632, 373, 407, 2408, 329, 606, 780, 484, 547, 7373, 290, 5742, 1123, 584, 13, 2293, 484, 5201, 11, 20037, 26280, 607, 1995, 329, 7373, 262, 17598, 290, 18682, 607, 10147, 13, 1119, 1111, 2936, 3772, 780, 484, 550, 4888, 290, 3111, 1978, 13]\n",
      "\n",
      "Decoded first 20: One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from pathlib import Path\n",
    "import struct\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Tokenize the story\n",
    "tokens = tokenizer.encode(test_story)\n",
    "\n",
    "print(f\"Token count: {len(tokens)}\")\n",
    "print(f\"First 20 tokens: {tokens[:1024]}\")\n",
    "\n",
    "# Verify decoding works\n",
    "decoded = tokenizer.decode(tokens[:1024])\n",
    "print(f\"\\nDecoded first 20: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfb50d6-82cb-46b6-b300-540c7719ce51",
   "metadata": {},
   "source": [
    "#### Pack stories until you hit a target token count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bed80783-4b8f-45aa-8333-fa7e187b5f08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packed 57 stories\n",
      "Total tokens: 10054\n",
      "Training pairs: 9030\n"
     ]
    }
   ],
   "source": [
    "target_tokens = 10000  # Aim for 10k tokens\n",
    "\n",
    "packed_stories = []\n",
    "current_tokens = 0\n",
    "\n",
    "for idx, story in enumerate(df['text']):\n",
    "    packed_stories.append(story)\n",
    "    current_tokens = len(tokenizer.encode(\"\\n\\n\".join(packed_stories)))\n",
    "    \n",
    "    if current_tokens >= target_tokens:\n",
    "        break\n",
    "\n",
    "packed_text = \"\\n\\n\".join(packed_stories)\n",
    "tokens = tokenizer.encode(packed_text)\n",
    "\n",
    "print(f\"Packed {len(packed_stories)} stories\")\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(f\"Training pairs: {len(tokens) - 1024}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34124aae-e152-41cc-a8d0-10549ac341c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 500 characters:\n",
      "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them b\n",
      "\n",
      "...\n",
      "\n",
      "Last 500 characters:\n",
      "\n",
      "The lawyer was very confused. He had never seen a little girl so eager to talk to him. He tried to explain but Mia kept on jumping and interrupting.\n",
      "\n",
      "Soon enough the store manager got involved. He explained to Mia that it was not appropriate to engage with strangers and it was wrong to interrupt people when they were talking.\n",
      "\n",
      "Mia was very sorry for her behaviour and decided to never do something like this again. She had learned her lesson that it is important to be respectful to all strangers.\n"
     ]
    }
   ],
   "source": [
    "# Preview the packed text\n",
    "print(\"First 500 characters:\")\n",
    "print(packed_text[:500])\n",
    "print(\"\\n...\")\n",
    "print(\"\\nLast 500 characters:\")\n",
    "print(packed_text[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed148532-8dd3-439a-b4d0-c2c5bda2f628",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 9030 training pairs\n",
      "Each pair: 1025 tokens\n",
      "  - Input:   tokens[0:1024]\n",
      "  - Targets: tokens[1:1025]\n"
     ]
    }
   ],
   "source": [
    "# ============ STEP 3: Create Training Pairs ============\n",
    "\n",
    "context_length = 1024\n",
    "\n",
    "training_pairs = []\n",
    "\n",
    "# Slide window by 1 token\n",
    "for i in range(len(tokens) - context_length):\n",
    "    # Extract 1025 tokens: [input[0:1024], target[1:1025]]\n",
    "    sequence = tokens[i:i+context_length+1]\n",
    "    \n",
    "    training_pairs.append({\n",
    "        'pair_id': i,\n",
    "        'tokens': sequence  # 1025 tokens\n",
    "    })\n",
    "\n",
    "df_pairs = pd.DataFrame(training_pairs)\n",
    "\n",
    "print(f\"Created {len(df_pairs)} training pairs\")\n",
    "print(f\"Each pair: 1025 tokens\")\n",
    "print(f\"  - Input:   tokens[0:1024]\")\n",
    "print(f\"  - Targets: tokens[1:1025]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "557bffb2-6a22-4515-ad3d-0e86b5e21ec3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Pair 0:\n",
      "============================================================\n",
      "Input: 1024 tokens\n",
      "First 10 tokens: [3198, 1110, 11, 257, 1310, 2576, 3706, 20037, 1043, 257]\n",
      "Decoded (first 100 chars): One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "\n",
      "============================================================\n",
      "Targets: 1024 tokens\n",
      "First 10 targets: [1110, 11, 257, 1310, 2576, 3706, 20037, 1043, 257, 17598]\n",
      "Decoded (first 100 chars):  day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "Model predicts:\n",
      "  After input[0] → should predict target[0] (token 1110)\n",
      "  After input[1] → should predict target[1] (token 11)\n",
      "  ...\n",
      "  After input[1023] → should predict target[1023] (token 679)\n"
     ]
    }
   ],
   "source": [
    "# Inspect first training pair\n",
    "pair = df_pairs.iloc[0]\n",
    "\n",
    "print(\"Training Pair 0:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show input tokens (first 1024)\n",
    "input_tokens = pair['tokens'][:1024]\n",
    "print(f\"Input: {len(input_tokens)} tokens\")\n",
    "print(f\"First 10 tokens: {input_tokens[:10]}\")\n",
    "print(f\"Decoded (first 100 chars): {tokenizer.decode(input_tokens[:50])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Show what model should predict at each position\n",
    "target_tokens = pair['tokens'][1:1025]\n",
    "print(f\"Targets: {len(target_tokens)} tokens\")\n",
    "print(f\"First 10 targets: {target_tokens[:10]}\")\n",
    "print(f\"Decoded (first 100 chars): {tokenizer.decode(target_tokens[:50])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Model predicts:\")\n",
    "print(f\"  After input[0] → should predict target[0] (token {target_tokens[0]})\")\n",
    "print(f\"  After input[1] → should predict target[1] (token {target_tokens[1]})\")\n",
    "print(f\"  ...\")\n",
    "print(f\"  After input[1023] → should predict target[1023] (token {target_tokens[1023]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3070f2e8-50da-40fe-a7c1-be196f6f4a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 9030 pairs to data/training_pairs\n",
      "  Saved 0/9030\n",
      "  Saved 1000/9030\n",
      "  Saved 2000/9030\n",
      "  Saved 3000/9030\n",
      "  Saved 4000/9030\n",
      "  Saved 5000/9030\n",
      "  Saved 6000/9030\n",
      "  Saved 7000/9030\n",
      "  Saved 8000/9030\n",
      "  Saved 9000/9030\n",
      "\n",
      "✓ Complete! Saved 9030 binary files\n"
     ]
    }
   ],
   "source": [
    "# ============ STEP 4: Save Training Pairs as Binary Files ============\n",
    "\n",
    "from pathlib import Path\n",
    "import struct\n",
    "\n",
    "pairs_dir = Path(\"./data/training_pairs\")\n",
    "pairs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Saving {len(df_pairs)} pairs to {pairs_dir}\")\n",
    "\n",
    "for idx, row in df_pairs.iterrows():\n",
    "    sequence = row['tokens']  # 1025 tokens\n",
    "    \n",
    "    # Save as binary: unsigned 32-bit integers\n",
    "    pair_file = pairs_dir / f\"pair_{idx:05d}.bin\"\n",
    "    with open(pair_file, 'wb') as f:\n",
    "        f.write(struct.pack(f'{len(sequence)}I', *sequence))\n",
    "    \n",
    "    if idx % 1000 == 0:\n",
    "        print(f\"  Saved {idx}/{len(df_pairs)}\")\n",
    "\n",
    "print(f\"\\n✓ Complete! Saved {len(df_pairs)} binary files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "498db5d1-6b5b-4cf9-a779-eb1e07946421",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1025 tokens from pair_00000.bin\n",
      "First 10: [3198, 1110, 11, 257, 1310, 2576, 3706, 20037, 1043, 257]\n",
      "Matches original: True\n"
     ]
    }
   ],
   "source": [
    "# ============ STEP 5: Verify Binary Files ============\n",
    "\n",
    "# Read back first file to verify\n",
    "test_file = pairs_dir / \"pair_00000.bin\"\n",
    "\n",
    "with open(test_file, 'rb') as f:\n",
    "    loaded = list(struct.unpack('1025I', f.read()))\n",
    "\n",
    "print(f\"Loaded {len(loaded)} tokens from {test_file.name}\")\n",
    "print(f\"First 10: {loaded[:10]}\")\n",
    "print(f\"Matches original: {loaded == df_pairs.iloc[0]['tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07783928-4f17-4f58-833f-6e02a34c67bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING DATA READY\n",
      "============================================================\n",
      "Total stories packed:  57\n",
      "Total tokens:          10054\n",
      "Training pairs:        9030\n",
      "Context length:        1024\n",
      "Tokens per file:       1025\n",
      "Files location:        data/training_pairs\n",
      "\n",
      "C Code should:\n",
      "  1. Load pair_XXXXX.bin (1025 tokens)\n",
      "  2. int input[1024] = tokens[0:1024]\n",
      "  3. int targets[1024] = tokens[1:1025]\n",
      "  4. forward(model, input, 1024)\n",
      "  5. loss = cross_entropy(model->logits, targets, 1024)\n",
      "  6. backward(model, targets, 1024)\n",
      "  7. optimizer_step(model)\n",
      "\n",
      "Expected epochs: 10-50\n",
      "Expected time per epoch: ~9030 steps\n"
     ]
    }
   ],
   "source": [
    "# ============ SUMMARY FOR C CODE ============\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING DATA READY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total stories packed:  {len(packed_stories)}\")\n",
    "print(f\"Total tokens:          {len(tokens)}\")\n",
    "print(f\"Training pairs:        {len(df_pairs)}\")\n",
    "print(f\"Context length:        {context_length}\")\n",
    "print(f\"Tokens per file:       1025\")\n",
    "print(f\"Files location:        {pairs_dir}\")\n",
    "print()\n",
    "print(\"C Code should:\")\n",
    "print(\"  1. Load pair_XXXXX.bin (1025 tokens)\")\n",
    "print(\"  2. int input[1024] = tokens[0:1024]\")\n",
    "print(\"  3. int targets[1024] = tokens[1:1025]\")\n",
    "print(\"  4. forward(model, input, 1024)\")\n",
    "print(\"  5. loss = cross_entropy(model->logits, targets, 1024)\")\n",
    "print(\"  6. backward(model, targets, 1024)\")\n",
    "print(\"  7. optimizer_step(model)\")\n",
    "print()\n",
    "print(f\"Expected epochs: 10-50\")\n",
    "print(f\"Expected time per epoch: ~{len(df_pairs)} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87cc8f8c-6e53-4525-8d5a-ceeac2db5c80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metadata to data/training_pairs/metadata.json\n",
      "{\n",
      "  \"num_stories\": 57,\n",
      "  \"num_tokens\": 10054,\n",
      "  \"num_pairs\": 9030,\n",
      "  \"context_length\": 1024,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"tokenizer\": \"gpt2\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ============ SAVE METADATA ============\n",
    "\n",
    "# Save metadata for reference\n",
    "metadata = {\n",
    "    'num_stories': len(packed_stories),\n",
    "    'num_tokens': len(tokens),\n",
    "    'num_pairs': len(df_pairs),\n",
    "    'context_length': context_length,\n",
    "    'vocab_size': 50257,\n",
    "    'tokenizer': 'gpt2'\n",
    "}\n",
    "\n",
    "import json\n",
    "metadata_file = pairs_dir / \"metadata.json\"\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Saved metadata to {metadata_file}\")\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2393e44a-5f54-46b4-af91-1be5ade30741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
